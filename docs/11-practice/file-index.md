建议使用编号系统管理项目文件。如`2022_network_analysis`项目的文件结构：

```bash
2022_network_analysis
  \-01_code
  	\-a01_analysis.ipynb
  	\-a02_visualize.ipynb
	\-d01_merge.ipynb
  	\-d10_panjiva.ipynb
	\-d20_compustat.ipynb
  \-02_rdata
    \-10_panjiva
	  \-panjiva_us_import_2019.csv
	  \-panjiva_us_export_2019.csv
	\-20_compustat
	  \-compustat_na.csv
	  \-compustat_global.csv
  \-03_wdata
  	\-0100_reg_firm_year.csv
	\-0102_reg_firm_quarter.csv
  	\-1000_panjiva_processed.csv
  	\-2000_compustat_processed.csv
  \-04_result
  	\-tab_baseline.rtf
  	\-fig_trend.svg
```

## 代码

代码存放于`01_code`。

- `a01` 表示分析（`a` 取analysis首字母）用的第一份代码
- `d10` 表示处理（`d` 取data首字母）`10` 系列的数据。
	* `d01` 等编号靠前的则是合并数据用的代码

!!! note "为什么合并数据的代码要单独放？"

	数据处理中，永远建议将“清洗”和“合并”分开。，在 `d10`、`d20` 等代码中只负责将杂乱的原始数据清洗干净，变成结构化的、可用的数据。而将这些清洗好的数据合并在一起用于分析，则统一放在 `d01` 等编号靠前的代码中进行。

	如果在 `d10` 这些清洗代码的过程中，混入合并其他数据（如 `d20`），将来如果 20 系列的数据出了问题，就需要切记在更正完 20 系列的数据后，检查并重新生成 10 系列的数据。正所谓“牵一发而动全身”。如果将合并集中在一起做，就可以最大程度降低数据生成过程的相互依赖性。

	这跟提高系统可维护性的程序设计思路是一样的：高内聚、低耦合。

## 原始数据

原始数据存放于 `02_rdata`，即从外部获取的原始数据，如 `raw_data.csv`。

尽量不要改变原始数据的原本命名（除非是WRDS或CSMAR这些数据平台导出的意义不明的文件名），方便以后定位每份原始数据的来源。

原始数据也尽量按照编号分文件夹放好，方便检索。

## 经处理数据

经处理的数据存放于`03_wdata`。

- 数据组成成分可以按 `10`、`20` 的前缀编写。
	* 可以按数据来源分类，比如 Panjiva 的数据统一为 `10` 前缀，Compustat 的数据统一为 `20` 前缀。
	* 也可以按主题分类，比如股价数据统一为 `10` 前缀，宏观数据统一为 `20` 前缀。
	* `10` 和 `20` 中间故意留空，是为了方便将来插入新的相关主题的新数据，比如后续引入了另一个供应链数据库 Revere，则可以编号为 `11` 系列。
- 分析用的数据以 `01-09` 开头，比如回归分析的数据是 `01`、可视化用的数据是 `02`。
- 后两位可以按顺序依次从 `00` 编至 `99`。

!!! note "这样编号的好处是什么？"

	- 编号前缀可以快速定位某个数据是被哪份代码处理的。（这一点对于项目管理最为重要，当一个数据出问题的时候，你永远可以通过数字编号很快的找到问题的根源）
		* `d10_panjiva.ipynb` 产生的数据编号是 `1000_panjiva_yearly.csv`、`1001_panjiva_monthly.csv` 等。
	- 编号后缀可以指明数据在项目处理流程中的先后关系。

## 结果

输出结果存放于`04_result`

- `tab`前缀用于表格
- `fig`前缀用于图片

!!! note "这样编号的好处是什么？"

	方便在LaTeX中调用。